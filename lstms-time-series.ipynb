{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom IPython.display import display\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, callbacks, Sequential, Input, Model\nfrom keras.layers import *\nimport pickle","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"state_names = [\"Alabama\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\", \"Idaho\", \"Illinois\", \"Indiana\", \n               \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \n               \"Nebraska\", \"Nevada\", \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \n               \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \n               \"West Virginia\", \"Wisconsin\", \"Wyoming\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"us_mean_temp = []\nus_minmax_temp = []\nus_precip = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = \"../input/time-series-synthetic-uci/\"\nfor i in state_names:\n    mean_temp = pd.read_csv(path + i.lower().replace(\" \",\"\") + \"_mean_temp.csv\")\n    minmax_temp = pd.read_csv(path + i.lower().replace(\" \",\"\") + \"_minmax_temp.csv\")\n    precip = pd.read_csv(path + i.lower().replace(\" \",\"\") + \"_precip.csv\")\n    \n    us_mean_temp.append(mean_temp)\n    us_minmax_temp.append(minmax_temp)\n    us_precip.append(precip)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"increasing_precip = [\"Alabama\", \"Arizona\", \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\", \"Illinois\",\n                    \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maryland\", \"Massachusetts\",\n                    \"Mississippi\", \"Missouri\", \"New Hampshire\", \"New Jersey\", \"New York\", \"North Carolina\",\n                    \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Pennsylvania\", \"South Dakota\", \"Tennessee\", \"Texas\",\n                    \"Virginia\", \"West Virginia\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"precip_pos = [state_names.index(i) for i in increasing_precip]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"precip_neg = [i for i in range(len(state_names)) if not i in precip_pos]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Texas Mean, Maximum, Minimum Temperature and Precipitation\neast_texas_mean = pd.read_csv(\"../input/time-series-synthetic-uci/east_texas_mean_summer_temp.csv\")\nedwards_plateau_mean = pd.read_csv(\"../input/time-series-synthetic-uci/edwards_plateau_mean_summer_temp.csv\")\nhigh_plains_mean = pd.read_csv(\"../input/time-series-synthetic-uci/high_plains_mean_summer_temp.csv\")\nlow_rolling_plains_mean = pd.read_csv(\"../input/time-series-synthetic-uci/low_rolling_plains_mean_summer_temp.csv\")\nlower_valley_mean = pd.read_csv(\"../input/time-series-synthetic-uci/lower_valley_mean_summer_temp.csv\")\nnorth_central_mean = pd.read_csv(\"../input/time-series-synthetic-uci/north_central_mean_summer_temp.csv\")\nsouth_central_mean = pd.read_csv(\"../input/time-series-synthetic-uci/south_central_mean_summer_temp.csv\")\nsouth_texas_mean = pd.read_csv(\"../input/time-series-synthetic-uci/south_texas_mean_summer_temp.csv\")\ntrans_pecos_mean = pd.read_csv(\"../input/time-series-synthetic-uci/trans_pecos_mean_summer_temp.csv\")\nupper_coast_mean = pd.read_csv(\"../input/time-series-synthetic-uci/upper_coast_mean_summer_temp.csv\")\n\neast_texas_minmax = pd.read_csv(\"../input/time-series-synthetic-uci/east_texas_minmax_summer_temp.csv\")\nedwards_plateau_minmax = pd.read_csv(\"../input/time-series-synthetic-uci/edwards_plateau_minmax_summer_temp.csv\")\nhigh_plains_minmax = pd.read_csv(\"../input/time-series-synthetic-uci/high_plains_minmax_summer_temp.csv\")\nlow_rolling_plains_minmax = pd.read_csv(\"../input/time-series-synthetic-uci/low_rolling_plains_minmax_summer_temp.csv\")\nlower_valley_minmax = pd.read_csv(\"../input/time-series-synthetic-uci/lower_valley_minmax_summer_temp.csv\")\nnorth_central_minmax = pd.read_csv(\"../input/time-series-synthetic-uci/north_central_minmax_summer_temp.csv\")\nsouth_central_minmax = pd.read_csv(\"../input/time-series-synthetic-uci/south_central_minmax_summer_temp.csv\")\nsouth_texas_minmax = pd.read_csv(\"../input/time-series-synthetic-uci/south_texas_minmax_summer_temp.csv\")\ntrans_pecos_minmax = pd.read_csv(\"../input/time-series-synthetic-uci/trans_pecos_minmax_summer_temp.csv\")\nupper_coast_minmax = pd.read_csv(\"../input/time-series-synthetic-uci/upper_coast_minmax_summer_temp.csv\")\n\neast_texas_precip = pd.read_csv(\"../input/time-series-synthetic-uci/east_texas_summer_precip.csv\")\nedwards_plateau_precip = pd.read_csv(\"../input/time-series-synthetic-uci/edwards_plateau_summer_precip.csv\")\nhigh_plains_precip = pd.read_csv(\"../input/time-series-synthetic-uci/high_plains_summer_precip.csv\")\nlow_rolling_plains_precip = pd.read_csv(\"../input/time-series-synthetic-uci/low_rolling_plains_summer_precip.csv\")\nlower_valley_precip = pd.read_csv(\"../input/time-series-synthetic-uci/lower_valley_summer_precip.csv\")\nnorth_central_precip = pd.read_csv(\"../input/time-series-synthetic-uci/north_central_summer_precip.csv\")\nsouth_central_precip = pd.read_csv(\"../input/time-series-synthetic-uci/south_central_summer_precip.csv\")\nsouth_texas_precip = pd.read_csv(\"../input/time-series-synthetic-uci/south_texas_summer_precip.csv\")\ntrans_pecos_precip = pd.read_csv(\"../input/time-series-synthetic-uci/trans_pecos_summer_precip.csv\")\nupper_coast_precip = pd.read_csv(\"../input/time-series-synthetic-uci/upper_coast_summer_precip.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tx_mean_temp = [east_texas_mean,edwards_plateau_mean,high_plains_mean,low_rolling_plains_mean,\n               lower_valley_mean,north_central_mean,south_central_mean,south_texas_mean,\n                trans_pecos_mean,upper_coast_mean]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tx_minmax_temp = [east_texas_minmax,edwards_plateau_minmax,high_plains_minmax,low_rolling_plains_minmax,\n               lower_valley_minmax,north_central_minmax,south_central_minmax,south_texas_minmax,\n                  trans_pecos_minmax,upper_coast_minmax]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tx_precip = [east_texas_precip,edwards_plateau_precip,high_plains_precip,low_rolling_plains_precip,\n            lower_valley_precip,north_central_precip,south_central_precip,south_texas_precip,\n             trans_pecos_precip,upper_coast_precip]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"names_tx = [\"East Texas\",\"Edwards Plateau\",\"High Plains\",\"Low Rolling Plains\",\"Lower Valley\",\n               \"North Central\",\"South Central\",\"South Texas\",\"Trans Pecos\",\"Upper Coast\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Takes ~20 seconds to run because inefficient\n\n# There is a high correlation between min/max temperatures and mean temperatures. \n# In fact, the assumption is that they differ by a constant amount throughout the time series\n# This is visually and numerically confirmed \ndef get_adjustments(data_list_minmax, data_list_mean, start, end):\n    adjustments = np.zeros((len(data_list_mean),2))\n    for ii in range(len(data_list_mean)):\n        lowest_error_min = np.inf\n        lowest_error_max = np.inf\n        best_adjustment_min = 0\n        best_adjustment_max = 0\n        for i in np.arange(start, end, 0.01):\n            error_min = np.mean(np.abs(data_list_minmax[ii].iloc[:,-1] + i - data_list_mean[ii].iloc[:,-1]))\n            error_max = np.mean(np.abs(data_list_minmax[ii].iloc[:,-2] - i - data_list_mean[ii].iloc[:,-1]))\n            if error_min < lowest_error_min:\n                lowest_error_min = error_min\n                best_adjustment_min = i\n            if error_max < lowest_error_max:\n                lowest_error_max = error_max\n                best_adjustment_max = i\n        adjustments[ii,0] = best_adjustment_max\n        adjustments[ii,1] = best_adjustment_min\n    return adjustments","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adjustments_tx = get_adjustments(tx_minmax_temp, tx_mean_temp, 5, 15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adjustments_us = get_adjustments(us_minmax_temp, us_mean_temp, 0, 20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colors=[\"tab:blue\",\"tab:orange\",\"tab:green\",\"tab:red\",\"tab:purple\",\"tab:brown\",\"tab:pink\",\"tab:gray\",\n       \"tab:olive\",\"tab:cyan\",\"k\",\"m\",\"yellow\",\"lightcoral\",\"darkblue\",\"lightgreen\",\"burlywood\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_trend(data_list, ewm, title, names):\n    plt.figure(figsize=(15,10))\n    for i in range(len(data_list)):\n        plt.plot(data_list[i].iloc[:,-1].ewm(ewm).mean().values, label=names[i], c=colors[i])\n        # plt.plot(data_list[i].iloc[:,-1].ewm(ewm).mean().values / 90, label=names[i], c=colors[i]) # uncomment for precipitation\n    plt.xticks(ticks=np.arange(1,46,5), labels=np.arange(1980,2025,5), fontsize=12)\n    plt.yticks(fontsize=12)\n    plt.ylabel(\"Temperature (Â°F.)\", fontsize=12)\n#     plt.ylabel(\"Precipitation (in.)\", fontsize=12) # uncomment for precipitation\n    plt.title(title + \"(1979-2021)\", fontsize=20)\n    plt.legend(bbox_to_anchor=(1.03,1), fontsize=12)\n#     plt.savefig(\"texas.png\",bbox_inches=\"tight\")\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_trend(tx_mean_temp, 5, \"Texas Climate Divisions Mean Summer Temperature\", names_tx)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def f(x):\n    return (1/100)*x - np.exp(-0.01*x)*np.sin(0.6*x)*0.5*x**0.3 + 81.5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(2,5,figsize=(20,7))\ncoords = [(i,j) for j in range(5) for i in range(2)]\nx = np.linspace(0,43,100)\nfor i in range(7,len(south_fl)):\n    if i == 7:\n        axs[coords[i-7]].plot(south_fl[i].iloc[:,-1].ewm(0).mean().values)\n        axs[coords[i-7]].plot(x, f(x), label=\"T(t)\")\n        axs[coords[i-7]].set_title(names[i],fontsize=14)\n        axs[coords[i-7]].legend(fontsize=14)\n    else:\n        axs[coords[i-7]].plot(south_fl[i].iloc[:,-1].ewm(0).mean().values)\n        axs[coords[i-7]].plot(x, f(x))\n        axs[coords[i-7]].set_title(names[i],fontsize=14)\n# plt.savefig(\"math_time_series_2.png\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lookback_len = 20 # how far to look back\npred_len = 10 # how many timesteps to predict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we need continuous sequences of length lookback_len and pred_len to serve as X and Y respectively \ndef create_examples(lookback_len, pred_len, data):\n    X = []\n    Y = []\n    for i in range(0, data.shape[0] - lookback_len - pred_len):\n        X.append(data[i:i+lookback_len].reshape(-1,1))\n        Y.append(data[i+lookback_len:i+lookback_len+pred_len].reshape(-1,1))\n    X = np.array(X)\n    Y = np.array(Y)\n    return X,Y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_data(ewm_len,data_list,lookback_len=20,pred_len=10):\n    X = [] \n    Y = []\n    for i in range(len(data_list)): # assembling X and Y using data from all counties\n        data = data_list[i].iloc[:,-1].ewm(ewm_len).mean().dropna().values\n        x, y = create_examples(lookback_len, pred_len, data)\n        X.append(x)\n        Y.append(y)\n    X = np.array(X)\n    X = X.reshape(X.shape[0]*X.shape[1],lookback_len,1)\n    \n    Y = np.array(Y)\n    Y = Y.reshape(Y.shape[0]*Y.shape[1],pred_len,1)\n    return X, Y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"variable = [us_precip[i] for i in precip_neg]\newm_len = 5\nX, Y = get_data(ewm_len,variable,lookback_len=lookback_len,pred_len=pred_len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_LSTM(input_shape, num_units, dropout_rate, pred_len, initializer):\n    input_layer = Input(shape=input_shape)\n    lstm1 = Dropout(dropout_rate)(input_layer)\n    lstm1 = LSTM(num_units, kernel_initializer=initializer)(lstm1)\n    lstm1 = Activation(\"tanh\")(lstm1)\n    output = Dense(pred_len)(lstm1)\n    model = Model(inputs=[input_layer], outputs=[output])\n    return model ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In each state/climate division, there are 43 data examples corresponding to 1979-2021,\n# meaning there are 43-(20+10)=13 time series sequences for each county.\n\n# We need to reserve the last few examples from each county as test data.\n# A random split cannot be used, as that would allow the model to see future data while training.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_idxs = []\ntest_idxs = []\n\nfor i in range(7,261,9): # for USA precip pos\n# for i in range(7,171,9): # for USA precip neg\n# for i in range(9,633,13): # for USA temp\n# for i in range(9,139,13): # for Texas\n    test_idxs += [i,i+1]\nfor i in range(X.shape[0]):\n    if not i in test_idxs:\n        train_idxs.append(i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_idxs) + len(test_idxs) == X.shape[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_t = X[train_idxs]\nX_v = X[test_idxs]\n\ny_t = Y[train_idxs]\ny_v = Y[test_idxs]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_t_means = np.mean(X_t, axis=1).reshape(-1,1,1)\nX_t_stds = np.std(X_t, axis=1).reshape(-1,1,1)\nX_t = (X_t - X_t_means) / X_t_stds\ny_t = (y_t - X_t_means) / X_t_stds\n\nX_v_means = np.mean(X_v, axis=1).reshape(-1,1,1)\nX_v_stds = np.std(X_v, axis=1).reshape(-1,1,1)\nX_v = (X_v - X_v_means) / X_v_stds\ny_v = (y_v - X_v_means) / X_v_stds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"initializer = tf.keras.initializers.GlorotNormal(seed=0)\n\nnum_units = 32\ndropout_rate = 0.2\n\nmodel = build_LSTM((X_t.shape[1:]), num_units, dropout_rate, pred_len, initializer)\nmodel.compile(optimizer=\"adam\", loss=\"mae\", metrics=[\"mse\"])\nhistory_nn = model.fit(X, Y, batch_size=8, epochs=75)\nhistory_nn = model.fit(X_t, y_t, validation_data=(X_v, y_v), batch_size=8, epochs=150)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(pd.DataFrame(history_nn.history).loss)\nplt.plot(pd.DataFrame(history_nn.history).val_loss)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = \"../input/time-series-synthetic-uci/florida_temp_weights.h5\"\n# model.save_weights(path)\nmodel.load_weights(path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.save_weights(\"us_precip_pos_weights.h5\")\n# model.save_weights(\"us_precip_neg_weights.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def forecast(data_list, names_list, num_rows, num_cols, idx_legend):\n    fig, axs = plt.subplots(num_rows,num_cols,figsize=(20,15))\n    coords = [(i,j) for j in range(num_cols) for i in range(num_rows)]\n    for county in range(len(data_list)):\n#     for county in precip_neg:\n        all_x = data_list[county].iloc[:,-1].ewm(0).mean().dropna().values\n        x = all_x[-lookback_len:]\n        means = np.mean(x)\n        stds = np.std(x)\n        x = (x - means) / stds\n        y = []   \n        for i in range(3): # 40 years into the future\n            pred = model.predict(x.reshape(1,lookback_len,1))\n            pred = pred * stds + means\n            x = x * stds + means\n            y.append(pred[0].flatten())\n            x = np.append(x.reshape(-1)[-10:], pred.reshape(-1))\n            means = np.mean(x)\n            stds = np.std(x)\n            x = (x - means) / stds\n#         y = np.array(y) / 90 # Uncomment for precipitation (3 months x 30 days per month)\n#         x = x / 90\n        axs[coords[county]].plot(np.arange(0,43), all_x, label=\"Past\")\n#         axs[coords[county]].plot(np.arange(4,43), all_x, label=\"Past\") when rolling=5 is used\n        axs[coords[county]].axvspan(42,72, facecolor='0.2', alpha=0.2)\n        axs[coords[county]].plot(np.arange(42,44), np.append(all_x[-1], np.array(y).flatten()[0]), c=\"tab:orange\")\n        axs[coords[county]].plot(np.arange(43,73), np.array(y).flatten().reshape(-1), label=\"Future\")\n        axs[coords[county]].set_title(names_list[county], fontsize=14)\n        axs[coords[county]].set_xticks(ticks=np.arange(2,82,10), labels=np.arange(1980,2060,10))\n        if county == idx_legend: \n            axs[coords[county]].legend(bbox_to_anchor=(1.03,1), fontsize=14)\n    plt.subplots_adjust(hspace=0.4)\n#     fig.delaxes(axs[5,2])\n#     plt.savefig(\"tx_predictions_precip.png\")\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"forecast(tx_precip, names_tx, 5, 2, 5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def forecast_df(data_list, names):\n    future = np.zeros((len(data_list),3*pred_len))\n    for county in range(len(data_list)):\n        all_x = data_list[county].iloc[:,-1].ewm(0).mean().dropna().values\n        x = all_x[-lookback_len:]\n        means = np.mean(x)\n        stds = np.std(x)\n        x = (x - means) / stds\n        y = []   \n        for i in range(3): # 30 years into the future\n            pred = model.predict(x.reshape(1,lookback_len,1))\n            pred = pred * stds + means\n            future[county,i*pred_len:(i+1)*pred_len] = pred.reshape(-1\n            x = x * stds + means\n            y.append(pred[0].flatten())\n            x = np.append(x.reshape(-1)[-10:], pred.reshape(-1))\n            means = np.mean(x)\n            stds = np.std(x)\n            x = (x - means) / stds\n    future = pd.DataFrame(future)\n    return future","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"us_future_mean_temp = forecast_df(us_mean_temp, state_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"us_future_precip = forecast_df(us_precip, state_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# temporary = forecast_df(us_precip, state_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(temporary.shape[0]):\n    temporary.iloc[i] = temporary.iloc[i].ewm(5).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for state in precip_neg:\n    lst = []\n    for i in range(8, 38, 10):\n        lst.append(temporary.iloc[state,i])\n    us_future_precip_decades.iloc[state,:-1] = lst  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(us_future_precip.shape[0]):\n    us_future_precip.iloc[i] = us_future_precip.iloc[i].ewm(5).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"us_future_precip_decades = np.zeros((48,3))\nfor state in precip_pos:\n    lst = []\n    for i in range(8, 38, 10):\n        lst.append(us_future_precip.iloc[state,i])\n    us_future_precip_decades[state] = lst  \nus_future_precip_decades = pd.DataFrame(us_future_precip_decades)\nus_future_precip_decades[\"State\"] = state_names\nus_future_precip_decades.columns = [2030,2040,2050,\"State\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"us_future_precip_decades.iloc[:,:-1] /= 90","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"us_future_mean_temp_decades = np.zeros((48,3))\nfor state in range(len(us_mean_temp)):\n    lst = []\n    for i in range(8, 38, 10):\n        lst.append(us_future_mean_temp.iloc[state,i])\n    us_future_mean_temp_decades[state] = lst  \nus_future_mean_temp_decades = pd.DataFrame(us_future_mean_temp_decades)\nus_future_mean_temp_decades[\"State\"] = state_names\nus_future_mean_temp_decades.columns = [2030,2040,2050,\"States\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"us_future_min_temp = us_future_mean_temp_decades.copy()\nus_future_max_temp = us_future_mean_temp_decades.copy()\n\nfor i in range(len(us_mean_temp)):\n    us_future_min_temp.iloc[i,:-1] -= adjustments_us[i,1]\n    us_future_max_temp.iloc[i,:-1] += adjustments_us[i,0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"us_future_mean_temp_decades.to_csv(\"us_mean_temp_predictions.csv\")\nus_future_min_temp.to_csv(\"us_min_temp_predictions.csv\")\nus_future_max_temp.to_csv(\"us_max_temp_predictions.csv\")\nus_future_precip_decades.to_csv(\"us_precip_predictions.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tx_future_mean_temp = forecast_df(tx_mean_temp, names_tx)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(tx_future_mean_temp.shape[0]):\n    tx_future_mean_temp.iloc[i] = tx_future_mean_temp.iloc[i].ewm(10).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tx_future_precip = forecast_df(tx_precip, names_tx)\ntx_future_precip /= 90","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(tx_future_precip.shape[0]):\n    tx_future_precip.iloc[i] = tx_future_precip.iloc[i].ewm(10).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tx_future_mean_temp_decades = np.zeros((10,4))\nfor county in range(len(tx_mean_temp)):\n    lst = []\n    for i in range(8, 48, 10):\n        lst.append(tx_future_mean_temp.iloc[county,i])\n    tx_future_mean_temp_decades[county] = lst  \ntx_future_mean_temp_decades = pd.DataFrame(tx_future_mean_temp_decades)\ntx_future_mean_temp_decades[\"Division\"] = names_tx\ntx_future_mean_temp_decades.columns = [2030,2040,2050,2060,\"Division\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tx_future_precip_decade = np.zeros((10,4))\nfor county in range(len(tx_precip)):\n    lst = []\n    for i in range(8, 48, 10):\n        lst.append(tx_future_precip.iloc[county,i])\n    tx_future_precip_decade[county] = lst  \ntx_future_precip_decade = pd.DataFrame(tx_future_precip_decade)\ntx_future_precip_decade[\"Division\"] = names_tx\ntx_future_precip_decade.columns = [2030,2040,2050,2060,\"Division\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tx_future_min_temp = tx_future_mean_temp_decades.copy()\ntx_future_max_temp = tx_future_mean_temp_decades.copy()\n\nfor i in range(len(tx_mean_temp)):\n    tx_future_min_temp.iloc[i,:-1] -= adjustments_tx[i,1]\n    tx_future_max_temp.iloc[i,:-1] += adjustments_tx[i,0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tx_future_min_temp = tx_future_mean_temp.copy()\ntx_future_max_temp = tx_future_mean_temp.copy()\n\nfor i in range(len(tx_mean_temp)):\n    tx_min_temp.iloc[i,:-1] -= adjustments_tx[i,1]\n    tx_max_temp.iloc[i,:-1] += adjustments_tx[i,0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tx_future_mean_temp_decades.to_csv(\"tx_mean_temp_predictions.csv\")\ntx_future_min_temp.to_csv(\"tx_min_temp_predictions.csv\")\ntx_future_max_temp.to_csv(\"tx_max_temp_predictions.csv\")\ntx_future_precip_decade.to_csv(\"tx_precip_predictions.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}