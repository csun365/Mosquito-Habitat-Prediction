{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks, Sequential, Input, Model\n",
    "from keras.layers import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_names = [\"Alabama\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\", \"Idaho\", \"Illinois\", \"Indiana\", \n",
    "               \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \n",
    "               \"Nebraska\", \"Nevada\", \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \n",
    "               \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \n",
    "               \"West Virginia\", \"Wisconsin\", \"Wyoming\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_mean_temp = []\n",
    "us_minmax_temp = []\n",
    "us_precip = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"\"\n",
    "for i in state_names:\n",
    "    mean_temp = pd.read_csv(path + i.lower().replace(\" \",\"\") + \"_mean_temp.csv\")\n",
    "    minmax_temp = pd.read_csv(path + i.lower().replace(\" \",\"\") + \"_minmax_temp.csv\")\n",
    "    precip = pd.read_csv(path + i.lower().replace(\" \",\"\") + \"_precip.csv\")\n",
    "    \n",
    "    us_mean_temp.append(mean_temp)\n",
    "    us_minmax_temp.append(minmax_temp)\n",
    "    us_precip.append(precip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "increasing_precip = [\"Alabama\", \"Arizona\", \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\", \"Illinois\",\n",
    "                    \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maryland\", \"Massachusetts\",\n",
    "                    \"Mississippi\", \"Missouri\", \"New Hampshire\", \"New Jersey\", \"New York\", \"North Carolina\",\n",
    "                    \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Pennsylvania\", \"South Dakota\", \"Tennessee\", \"Texas\",\n",
    "                    \"Virginia\", \"West Virginia\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_pos = [state_names.index(i) for i in increasing_precip]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_neg = [i for i in range(len(state_names)) if not i in precip_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_tx = [\"East Texas\",\"Edwards Plateau\",\"High Plains\",\"Low Rolling Plains\",\"Lower Valley\",\n",
    "               \"North Central\",\"South Central\",\"South Texas\",\"Trans Pecos\",\"Upper Coast\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_mean_temp = []\n",
    "tx_minmax_temp = []\n",
    "tx_precip = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"\"\n",
    "for i in names_tx:\n",
    "    mean_temp = pd.read_csv(path + i.lower().replace(\" \",\"\") + \"_mean_temp.csv\")\n",
    "    minmax_temp = pd.read_csv(path + i.lower().replace(\" \",\"\") + \"_minmax_temp.csv\")\n",
    "    precip = pd.read_csv(path + i.lower().replace(\" \",\"\") + \"_precip.csv\")\n",
    "    \n",
    "    tx_mean_temp.append(mean_temp)\n",
    "    tx_minmax_temp.append(minmax_temp)\n",
    "    tx_precip.append(precip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjustments(data_list_minmax, data_list_mean, start, end):\n",
    "    adjustments = np.zeros((len(data_list_mean),2))\n",
    "    for ii in range(len(data_list_mean)):\n",
    "        lowest_error_min = np.inf\n",
    "        lowest_error_max = np.inf\n",
    "        best_adjustment_min = 0\n",
    "        best_adjustment_max = 0\n",
    "        for i in np.arange(start, end, 0.01):\n",
    "            error_min = np.mean(np.abs(data_list_minmax[ii].iloc[:,-1] + i - data_list_mean[ii].iloc[:,-1]))\n",
    "            error_max = np.mean(np.abs(data_list_minmax[ii].iloc[:,-2] - i - data_list_mean[ii].iloc[:,-1]))\n",
    "            if error_min < lowest_error_min:\n",
    "                lowest_error_min = error_min\n",
    "                best_adjustment_min = i\n",
    "            if error_max < lowest_error_max:\n",
    "                lowest_error_max = error_max\n",
    "                best_adjustment_max = i\n",
    "        adjustments[ii,0] = best_adjustment_max\n",
    "        adjustments[ii,1] = best_adjustment_min\n",
    "    return adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjustments_tx = get_adjustments(tx_minmax_temp, tx_mean_temp, 5, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjustments_us = get_adjustments(us_minmax_temp, us_mean_temp, 0, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors=[\"tab:blue\",\"tab:orange\",\"tab:green\",\"tab:red\",\"tab:purple\",\"tab:brown\",\"tab:pink\",\"tab:gray\",\n",
    "       \"tab:olive\",\"tab:cyan\",\"k\",\"m\",\"yellow\",\"lightcoral\",\"darkblue\",\"lightgreen\",\"burlywood\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trend(data_list, ewm, title, names):\n",
    "    plt.figure(figsize=(15,10))\n",
    "    for i in range(len(data_list)):\n",
    "        plt.plot(data_list[i].iloc[:,-1].ewm(ewm).mean().values, label=names[i], c=colors[i])\n",
    "        # plt.plot(data_list[i].iloc[:,-1].ewm(ewm).mean().values / 90, label=names[i], c=colors[i]) # uncomment for precipitation\n",
    "    plt.xticks(ticks=np.arange(1,46,5), labels=np.arange(1980,2025,5), fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.ylabel(\"Temperature (Â°F.)\", fontsize=12)\n",
    "#     plt.ylabel(\"Precipitation (in.)\", fontsize=12) # uncomment for precipitation\n",
    "    plt.title(title + \"(1979-2021)\", fontsize=20)\n",
    "    plt.legend(bbox_to_anchor=(1.03,1), fontsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trend(tx_mean_temp, 5, \"Texas Climate Divisions Mean Summer Temperature\", names_tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return (1/100)*x - np.exp(-0.01*x)*np.sin(0.6*x)*0.5*x**0.3 + 81.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,5,figsize=(20,7))\n",
    "coords = [(i,j) for j in range(5) for i in range(2)]\n",
    "x = np.linspace(0,43,100)\n",
    "for i in range(7,len(south_fl)):\n",
    "    if i == 7:\n",
    "        axs[coords[i-7]].plot(south_fl[i].iloc[:,-1].ewm(0).mean().values)\n",
    "        axs[coords[i-7]].plot(x, f(x), label=\"T(t)\")\n",
    "        axs[coords[i-7]].set_title(names[i],fontsize=14)\n",
    "        axs[coords[i-7]].legend(fontsize=14)\n",
    "    else:\n",
    "        axs[coords[i-7]].plot(south_fl[i].iloc[:,-1].ewm(0).mean().values)\n",
    "        axs[coords[i-7]].plot(x, f(x))\n",
    "        axs[coords[i-7]].set_title(names[i],fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback_len = 20 # how far to look back\n",
    "pred_len = 10 # how many timesteps to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need continuous sequences of length lookback_len and pred_len to serve as X and Y respectively \n",
    "def create_examples(lookback_len, pred_len, data):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in range(0, data.shape[0] - lookback_len - pred_len):\n",
    "        X.append(data[i:i+lookback_len].reshape(-1,1))\n",
    "        Y.append(data[i+lookback_len:i+lookback_len+pred_len].reshape(-1,1))\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(ewm_len,data_list,lookback_len=20,pred_len=10):\n",
    "    X = [] \n",
    "    Y = []\n",
    "    for i in range(len(data_list)): # assembling X and Y using data from all counties\n",
    "        data = data_list[i].iloc[:,-1].ewm(ewm_len).mean().dropna().values\n",
    "        x, y = create_examples(lookback_len, pred_len, data)\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    X = np.array(X)\n",
    "    X = X.reshape(X.shape[0]*X.shape[1],lookback_len,1)\n",
    "    \n",
    "    Y = np.array(Y)\n",
    "    Y = Y.reshape(Y.shape[0]*Y.shape[1],pred_len,1)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = [us_precip[i] for i in precip_neg]\n",
    "ewm_len = 5\n",
    "X, Y = get_data(ewm_len,variable,lookback_len=lookback_len,pred_len=pred_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_LSTM(input_shape, num_units, dropout_rate, pred_len, initializer):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    lstm1 = Dropout(dropout_rate)(input_layer)\n",
    "    lstm1 = LSTM(num_units, kernel_initializer=initializer)(lstm1)\n",
    "    lstm1 = Activation(\"tanh\")(lstm1)\n",
    "    output = Dense(pred_len)(lstm1)\n",
    "    model = Model(inputs=[input_layer], outputs=[output])\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In each state/climate division, there are 43 data examples corresponding to 1979-2021,\n",
    "# meaning there are 43-(20+10)=13 time series sequences for each county.\n",
    "\n",
    "# We need to reserve the last few examples from each county as test data.\n",
    "# A random split cannot be used, as that would allow the model to see future data while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxs = []\n",
    "test_idxs = []\n",
    "\n",
    "for i in range(7,261,9): # for USA precip pos\n",
    "# for i in range(7,171,9): # for USA precip neg\n",
    "# for i in range(9,633,13): # for USA temp\n",
    "# for i in range(9,139,13): # for Texas\n",
    "    test_idxs += [i,i+1]\n",
    "for i in range(X.shape[0]):\n",
    "    if not i in test_idxs:\n",
    "        train_idxs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_idxs) + len(test_idxs) == X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = X[train_idxs]\n",
    "X_v = X[test_idxs]\n",
    "\n",
    "y_t = Y[train_idxs]\n",
    "y_v = Y[test_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t_means = np.mean(X_t, axis=1).reshape(-1,1,1)\n",
    "X_t_stds = np.std(X_t, axis=1).reshape(-1,1,1)\n",
    "X_t = (X_t - X_t_means) / X_t_stds\n",
    "y_t = (y_t - X_t_means) / X_t_stds\n",
    "\n",
    "X_v_means = np.mean(X_v, axis=1).reshape(-1,1,1)\n",
    "X_v_stds = np.std(X_v, axis=1).reshape(-1,1,1)\n",
    "X_v = (X_v - X_v_means) / X_v_stds\n",
    "y_v = (y_v - X_v_means) / X_v_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "initializer = tf.keras.initializers.GlorotNormal(seed=0)\n",
    "\n",
    "num_units = 32\n",
    "dropout_rate = 0.2\n",
    "\n",
    "model = build_LSTM((X_t.shape[1:]), num_units, dropout_rate, pred_len, initializer)\n",
    "model.compile(optimizer=\"adam\", loss=\"mae\", metrics=[\"mse\"])\n",
    "history_nn = model.fit(X, Y, batch_size=8, epochs=75)\n",
    "history_nn = model.fit(X_t, y_t, validation_data=(X_v, y_v), batch_size=8, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pd.DataFrame(history_nn.history).loss)\n",
    "plt.plot(pd.DataFrame(history_nn.history).val_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"\"\n",
    "# model.save_weights(path)\n",
    "# model.load_weights(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast(data_list, names_list, num_rows, num_cols, idx_legend):\n",
    "    fig, axs = plt.subplots(num_rows,num_cols,figsize=(20,15))\n",
    "    coords = [(i,j) for j in range(num_cols) for i in range(num_rows)]\n",
    "    for county in range(len(data_list)):\n",
    "#     for county in precip_neg:\n",
    "        all_x = data_list[county].iloc[:,-1].ewm(0).mean().dropna().values\n",
    "        x = all_x[-lookback_len:]\n",
    "        means = np.mean(x)\n",
    "        stds = np.std(x)\n",
    "        x = (x - means) / stds\n",
    "        y = []   \n",
    "        for i in range(3): # 40 years into the future\n",
    "            pred = model.predict(x.reshape(1,lookback_len,1))\n",
    "            pred = pred * stds + means\n",
    "            x = x * stds + means\n",
    "            y.append(pred[0].flatten())\n",
    "            x = np.append(x.reshape(-1)[-10:], pred.reshape(-1))\n",
    "            means = np.mean(x)\n",
    "            stds = np.std(x)\n",
    "            x = (x - means) / stds\n",
    "#         y = np.array(y) / 90 # Uncomment for precipitation (3 months x 30 days per month)\n",
    "#         x = x / 90\n",
    "        axs[coords[county]].plot(np.arange(0,43), all_x, label=\"Past\")\n",
    "#         axs[coords[county]].plot(np.arange(4,43), all_x, label=\"Past\") when rolling=5 is used\n",
    "        axs[coords[county]].axvspan(42,72, facecolor='0.2', alpha=0.2)\n",
    "        axs[coords[county]].plot(np.arange(42,44), np.append(all_x[-1], np.array(y).flatten()[0]), c=\"tab:orange\")\n",
    "        axs[coords[county]].plot(np.arange(43,73), np.array(y).flatten().reshape(-1), label=\"Future\")\n",
    "        axs[coords[county]].set_title(names_list[county], fontsize=14)\n",
    "        axs[coords[county]].set_xticks(ticks=np.arange(2,82,10), labels=np.arange(1980,2060,10))\n",
    "        if county == idx_legend: \n",
    "            axs[coords[county]].legend(bbox_to_anchor=(1.03,1), fontsize=14)\n",
    "    plt.subplots_adjust(hspace=0.4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast(tx_precip, names_tx, 5, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_df(data_list, names):\n",
    "    future = np.zeros((len(data_list),3*pred_len))\n",
    "    for county in range(len(data_list)):\n",
    "        all_x = data_list[county].iloc[:,-1].ewm(0).mean().dropna().values\n",
    "        x = all_x[-lookback_len:]\n",
    "        means = np.mean(x)\n",
    "        stds = np.std(x)\n",
    "        x = (x - means) / stds\n",
    "        y = []   \n",
    "        for i in range(3): # 30 years into the future\n",
    "            pred = model.predict(x.reshape(1,lookback_len,1))\n",
    "            pred = pred * stds + means\n",
    "            future[county,i*pred_len:(i+1)*pred_len] = pred.reshape(-1\n",
    "            x = x * stds + means\n",
    "            y.append(pred[0].flatten())\n",
    "            x = np.append(x.reshape(-1)[-10:], pred.reshape(-1))\n",
    "            means = np.mean(x)\n",
    "            stds = np.std(x)\n",
    "            x = (x - means) / stds\n",
    "    future = pd.DataFrame(future)\n",
    "    return future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_future_mean_temp = forecast_df(us_mean_temp, state_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_future_precip = forecast_df(us_precip, state_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(us_future_precip.shape[0]):\n",
    "    us_future_precip.iloc[i] = us_future_precip.iloc[i].ewm(5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_future_precip_decades = np.zeros((48,3))\n",
    "for state in precip_pos:\n",
    "    lst = []\n",
    "    for i in range(8, 38, 10):\n",
    "        lst.append(us_future_precip.iloc[state,i])\n",
    "    us_future_precip_decades[state] = lst  \n",
    "us_future_precip_decades = pd.DataFrame(us_future_precip_decades)\n",
    "us_future_precip_decades[\"State\"] = state_names\n",
    "us_future_precip_decades.columns = [2030,2040,2050,\"State\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go back and generate predictions for states with decreasing precipitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporary = forecast_df(us_precip, state_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(temporary.shape[0]):\n",
    "    temporary.iloc[i] = temporary.iloc[i].ewm(5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in precip_neg:\n",
    "    lst = []\n",
    "    for i in range(8, 38, 10):\n",
    "        lst.append(temporary.iloc[state,i])\n",
    "    us_future_precip_decades.iloc[state,:-1] = lst  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_future_precip_decades.iloc[:,:-1] /= 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_future_mean_temp_decades = np.zeros((48,3))\n",
    "for state in range(len(us_mean_temp)):\n",
    "    lst = []\n",
    "    for i in range(8, 38, 10):\n",
    "        lst.append(us_future_mean_temp.iloc[state,i])\n",
    "    us_future_mean_temp_decades[state] = lst  \n",
    "us_future_mean_temp_decades = pd.DataFrame(us_future_mean_temp_decades)\n",
    "us_future_mean_temp_decades[\"State\"] = state_names\n",
    "us_future_mean_temp_decades.columns = [2030,2040,2050,\"States\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_future_min_temp = us_future_mean_temp_decades.copy()\n",
    "us_future_max_temp = us_future_mean_temp_decades.copy()\n",
    "\n",
    "for i in range(len(us_mean_temp)):\n",
    "    us_future_min_temp.iloc[i,:-1] -= adjustments_us[i,1]\n",
    "    us_future_max_temp.iloc[i,:-1] += adjustments_us[i,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"\"\n",
    "us_future_mean_temp_decades.to_csv(path)\n",
    "us_future_min_temp.to_csv(path)\n",
    "us_future_max_temp.to_csv(path)\n",
    "us_future_precip_decades.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_future_mean_temp = forecast_df(tx_mean_temp, names_tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(tx_future_mean_temp.shape[0]):\n",
    "    tx_future_mean_temp.iloc[i] = tx_future_mean_temp.iloc[i].ewm(10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_future_precip = forecast_df(tx_precip, names_tx)\n",
    "tx_future_precip /= 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(tx_future_precip.shape[0]):\n",
    "    tx_future_precip.iloc[i] = tx_future_precip.iloc[i].ewm(10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_future_mean_temp_decades = np.zeros((10,4))\n",
    "for county in range(len(tx_mean_temp)):\n",
    "    lst = []\n",
    "    for i in range(8, 48, 10):\n",
    "        lst.append(tx_future_mean_temp.iloc[county,i])\n",
    "    tx_future_mean_temp_decades[county] = lst  \n",
    "tx_future_mean_temp_decades = pd.DataFrame(tx_future_mean_temp_decades)\n",
    "tx_future_mean_temp_decades[\"Division\"] = names_tx\n",
    "tx_future_mean_temp_decades.columns = [2030,2040,2050,2060,\"Division\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_future_precip_decade = np.zeros((10,4))\n",
    "for county in range(len(tx_precip)):\n",
    "    lst = []\n",
    "    for i in range(8, 48, 10):\n",
    "        lst.append(tx_future_precip.iloc[county,i])\n",
    "    tx_future_precip_decade[county] = lst  \n",
    "tx_future_precip_decade = pd.DataFrame(tx_future_precip_decade)\n",
    "tx_future_precip_decade[\"Division\"] = names_tx\n",
    "tx_future_precip_decade.columns = [2030,2040,2050,2060,\"Division\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_future_min_temp = tx_future_mean_temp_decades.copy()\n",
    "tx_future_max_temp = tx_future_mean_temp_decades.copy()\n",
    "\n",
    "for i in range(len(tx_mean_temp)):\n",
    "    tx_future_min_temp.iloc[i,:-1] -= adjustments_tx[i,1]\n",
    "    tx_future_max_temp.iloc[i,:-1] += adjustments_tx[i,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_future_min_temp = tx_future_mean_temp.copy()\n",
    "tx_future_max_temp = tx_future_mean_temp.copy()\n",
    "\n",
    "for i in range(len(tx_mean_temp)):\n",
    "    tx_min_temp.iloc[i,:-1] -= adjustments_tx[i,1]\n",
    "    tx_max_temp.iloc[i,:-1] += adjustments_tx[i,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"\"\n",
    "tx_future_mean_temp_decades.to_csv(path)\n",
    "tx_future_min_temp.to_csv(path)\n",
    "tx_future_max_temp.to_csv(path)\n",
    "tx_future_precip_decades.to_csv(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
